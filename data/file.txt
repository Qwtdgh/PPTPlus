0:00
Welcome back. Last time, we introduced some important concepts in our theoretical
0:25
development. And the first concept was dichotomies.
0:31
And the idea is that there is an input space behind this opaque sheet, and there is a hypothesis that's separating red
0:38
regions from blue regions. But we don't get to see that. What we get to see are just the data points, holes in that
0:46
sheet if you will. And there could be very exciting stuff happening behind that sheet, and all
0:52
you get to see is when the boundary crosses one of these points, and a blue point turns red or vice-versa.
0:58
So if you think of the purpose for the dichotomies, we had a problem with counting the number of hypotheses, because we end up with
1:06
a very large number. But if you restrict your attention to dichotomies, which are the hypotheses
1:11
restricted to a finite set of points, the blue and red points here, then you
1:17
don't have to count everything that is happening outside. You only count it as different when something different happens only on
1:22
those points. So a dichotomy is a mini-hypothesis, if you will. And it counts the hypotheses only on the finite set of points.
1:34
This resulted in a definition that parallels the number of hypotheses, which is the number of dichotomies in this case.
1:41
So we define the growth function. The growth function is-- you pick the points x_1 up to x_N.
1:48
You pick them wisely, with a view to maximizing the dichotomies, such that the number you get will be more than any number another person gets with N
1:56
points. That's the purpose. So you take your hypothesis set, which applies to the entire input
2:02
space, and then apply it only to x_1 up to x_N.
2:07
This will result in a pattern of +1 or -1's, N of them. And as you vary the hypothesis within this set, you will get another
2:14
pattern, another pattern, another pattern. So you will get a set of different patterns that are all the dichotomies
2:20
that can be generated by this hypothesis set, on this set of points. And the number of those guys is what we are interested in.
2:27
It will play the role of the number of hypotheses. And that is the growth function.
2:33
Now in principle, the growth function can be 2 to the N. You may be in an input space and a hypothesis set, such that you can generate
2:40
any pattern you want. However, in most of the cases, the restriction of using hypotheses coming
2:46
from H will result in missing out on some of the patterns. Some patterns will simply be impossible.
2:52
And that led us to the idea of a break point. For the case of a perceptron in two dimensions, which is the case we
2:58
studied, we realize that for four points, there will always be a pattern that cannot be realized by a perceptron.
3:05
There is no way to have a line come here, and separate those red points from the blue points.
3:10
And any choice of four points will also result in missing patterns.
3:16
Therefore, the number k equals 4, in this case, is defined as a break point for the perceptrons.
3:22
And our theoretical goal is to take that single number, which is the break point, and be able to characterize the entire growth function for every N.
3:31
And therefore, be able to characterize the generalization, as we will see.
3:38
We then talked about the maximum number of dichotomies, under the constraint that there is a break point.
3:43
And we had an illustrative example to tell you that, when you tell me that you cannot get all patterns on any--
3:49
in this case, two points-- that is a very strong restriction on the number of dichotomies you can
3:55
get on a larger number of points. So this is the simplest case. If you take any two columns, you cannot get all four patterns.
4:02
That's by decree. I'm telling you that the hypothesis has a break point of 2. And then I'm asking you, under those constraints, how many lines you can
4:10
get, how many different patterns you can get. And you go and you add them up, and you end up in this
4:15
case with only four. So you lost half of them. And you can see that if we have 10 points, and you apply the same
4:20
restriction, there will be so many lost, because now the restriction applies to any pair of points.
4:26
Now, if you look at this schedule, this does not appeal to any particulars of the hypothesis set or the input space, other than the fact
4:34
that the break point is 2. I could be in a situation, where the hypothesis set cannot generate some of
4:40
these guys for other reasons. But here, I'm abstracting only a hypothesis set and an input space.
4:46
I don't want to bother to know more about them. Just tell me that they have a break point, and I'm trying to find under
4:52
that single constraint, how many can I possibly have? And I already have, by that combinatorial constraint,
4:58
a restriction which is strong enough to get me a good-enough result. That's good, because now I don't have to worry about every hypothesis
5:05
set, and every input space, you give me. I just ask you: what is the break point? And I'm able to make a statement about the growth function not being bigger
5:12
than something. That is the key. We move on to today's lecture, and the title is, properly, the Theory of
5:19
Generalization. It's very theoretical. And today's lecture is the most theoretical of the entire course.
5:26
So fasten your seat belts, and let's start!
5:31
We have two items of business. The first one is to show that the growth function, with a break point, is
5:38
indeed polynomial. The second one is to show that we can actually take that notion, the growth
5:45
function, and put it in place of M, the number of hypotheses, in Hoeffding's inequality.
5:52
So basically, we are saying in the first part: it's worthwhile to study
5:57
the growth function. Because being polynomial will be very advantageous. And then, the second one is: we can actually do something good with it.
6:05
We can do the replacement. These are the only two items. Let's start.
6:12
We are going to bound the growth function by a polynomial. And I just wanted to point some of the aspects of that.
6:20
If I say m_H of N is polynomial, it's not that I am going to actually solve for the growth function, and show that it is this particular polynomial, and
6:28
the coefficients. All I am saying is that it is really just bounded above by a polynomial.
6:36
I don't have to get the particulars of m_H of N, the growth function. I am going to just tell you that this is less than something, less than
6:42
something, less than a polynomial. That's all I need, because eventually I am going to put this in the
6:47
Hoeffding inequality. And as long as it's bounded by a polynomial, I am in business. Because the negative exponential will kill it, as we
6:53
discussed, and we are OK. So we can be a bit loose, which is very good in theory. Because now you leave a lot of artifacts that you
7:01
don't need to study. And just talk about the upper bound in the general case, and still get what you want to get.
7:07
The key quantity we are going to use, which is a purely combinatorial
7:12
quantity, we are going to call it B of N and k.
7:17
This is exactly the quantity we were seeking in the puzzle. I give you N points.
7:23
I tell you that k is a break point, and ask you: how many different patterns can you get under those conditions?
7:31
In that case, we had three points and the break point was 2. And we answered this question by construction.
7:38
We played around with the patterns until we got it, and then we said it's 4. Now, as I develop the theory, the puzzle will come up
7:45
in one of the results. I would like you to keep an eye and say which slide, and which particular
7:51
part of the slide, addresses the very specific puzzle we talked about.
7:57
The definition here is the maximum number of dichotomies on N points,
8:03
such that they have a break point k. So this is N and this is k.
8:09
And the good thing here is that I didn't appeal to any hypothesis set, or any input space.
8:14
This is a purely combinatorial quantity. And because it's a combinatorial quantity, I am going to be able to pin it down exactly, as it turns out.
8:22
And now, when I pin it down exactly, you go and you find the fanciest input space, and the fanciest hypothesis set.
8:28
You pick the break point for that, and you use that here, ridding the
8:35
problem of all the other aspects, and you still are able to make an upper bound statement. You can say that the growth function, for the particular case you talked
8:43
about, is less than or equal to-- and just go to this combinatorial quantity. The plan is clear.
8:49
So let's look at the bound for B of N, k. And we are going to do it recursively. It's a very cute argument, and I am going to build it very carefully.
8:57
So I want your attention. Consider the following table. Very much like the puzzle, we are going to list x_1, x_2, up to x_N.
9:07
N points, which used to be three points. And I am going to try to put as many patterns as I can, under a constraint
9:13
that there is a break point. So I will be putting the first pattern this way, and the second pattern, and so
9:21
on, trying to fill this table. Now, I am going to do a structural analysis of this, and this will happen
9:31
through this division. Let's look at it. Still the same problem, x_1 and x_N is my vector.
9:37
And I am trying to fill this with as many rows as possible, under a constraint of a break point. But now I am going to isolate the last point.
9:45
Why am I isolating the last point? Because I want a recursion. I want to be able to relate this fellow, to the same fellow applied to
9:52
smaller quantities. And you have seen enough of that to realize that, if I manage to do that, I might be able to actually solve for B of N and k.
10:00
That's why I'm isolating the last point. After I do the isolation, I am going to group the rows of the big
10:10
matrix, into some groups. This is just my way of looking at things.
10:15
I haven't changed anything. What I am going to do, I am going to shuffle the rows around, after you have
10:22
constructed them. So we have a full matrix now, and I am shuffling them, and putting some guys in the first group.
10:29
And the first group I am going to call S_1. Here is the definition of the group S_1.
10:36
These are the rows that appear only once, as far as x_1 up to
10:42
x_N-1 are concerned. Well, every row in its entirety appears only once, because these are
10:50
different rows. That's how I'm constructing the matrix. But if you take out the last guy, it is conceivable that the first N minus
10:57
1 coordinates happen twice, once with extension -1, once with extension +1.
11:03
So I am taking the guys that go with only one extension, whatever it might be. Could be -1 or could be +1, but not both, and putting
11:10
them in this group. Fairly well defined.
11:16
So you fill it up, and these are all the rows that have a single extension.
11:22
Now, you go under this, and you define the number of rows in
11:28
this group to be alpha. It is a number. I am just going to call it alpha. And you can see where this is going, because now I'm going to claim that
11:36
the B of N and k, which is the total number of rows in the entire matrix, is alpha plus something.
11:42
That is obvious. I have already taken care of alpha, and I am going to add up the other stuff later on.
11:49
So what is the other stuff? That is the stuff I am going to call S_2. And you probably have a good guess what these are.
11:56
These are the guys that happen with both patterns.
12:01
That is, they happen with extension +1 and with -1. That is disjoint from the first group.
12:08
A typical member will look like this. This is the same guy from x_1 up to x_N-1, as it appears here.
12:16
It just appears here with +1, and appears here with -1.
12:22
And I keep doing it. So what I'm doing, I just reorganize the rows of the matrix to fall into these nice categories.
12:27
The other guy? Exactly the same thing. So the second one corresponds to the second one, and so on.
12:34
Now, that covers all the rows. I look at x_1 up to x_N-1. I either have both extensions, or one extension. That's it.
12:42
One extension belongs to the first group. Two extensions belong to the second group in both ways, with +1 and -1.
12:49
In terms of the counting, this has beta rows, whatever beta might be. This also has beta rows, because they're identical.
12:56
And therefore, the number B of N and K, which I'm interested in, is
13:02
alpha plus 2 beta. That is complete. Just calling things names.
13:10
So now, I am going to try to find a handle on alphas and betas, so that I can find a recursion for the big function B of N and k.
13:19
B of N and k are the maximum number of rows-- patterns I can get on N points, such that no k columns have
13:30
all possible patterns. That's the definition. I am going to relate that to the same quantity on smaller numbers, smaller N
13:37
and smaller k. So the first is to estimate alpha and beta.
13:43
I'd like to ask you to focus on the x_1 up to x_N-1 columns.
13:51
And I am going to help you visually do that, by graying out the rest.
13:57
Now for a moment, look at these. Are these rows different?
14:04
They used to be different when you have the extension. Well, let me see. The first group, I know they are different, because
14:11
they have one extension. If there is one which is repeated, then it must be repeated with both
14:17
extensions, in order to get different rows all over, and that violates the condition for being here.
14:23
They are here because they have only one extension. These guys are the same.
14:28
This one appears with -1, and here appears with +1. But if you cut the last guy, this guy is identical to this guy, right?
14:39
This second guy is identical to the second guy. So I cannot count these as different rows.
14:45
I can do that when I gray out one of the groups.
14:51
Now, these are patently different. Nothing here is repeated, because we said they have only one extension, and
14:58
they are all tucked in here. These two guys, there are no two guys here that are equal, because they all
15:03
have the same extension. And supposedly, the whole row makes it different rows.
15:09
Therefore, these guys are different from each other. And these guys are different from here because again, if they are equal, then
15:16
I will have an extension. And then the guys here will belong to a row that had both extensions.
15:22
Very easy. Just a verbose argument, but we end up with these guys being different.
15:27
Now, I like the fact that these guys are being different, because when they are different, I can relate them to B of N and k.
15:33
B of N and k was the maximum number of patterns-- different rows, that's how I am counting them--
15:39
such that a condition occurs. So what is the condition that is occurring here? I can say that alpha plus beta, which is the total number of rows or
15:49
patterns in this mini-matrix, can I say something about a break point for
15:55
this small matrix? Yeah. The original matrix, I could not find all possible
16:02
patterns on any k columns, right? So I cannot possibly find all possible patterns on any k columns on this
16:10
smaller set. Because if I find all possible patterns on k columns here, they will
16:15
serve as all possible patterns in the big matrix. And I know, that doesn't exist.
16:21
So I can now confidently say that alpha plus beta, which is the number of different guys here, is less than or equal to B of N minus 1, because I
16:31
have only x_1 up to x_N-1, and k, because that is the break point for these guys as well.
16:39
Why am I saying less than or equal to, not equal? When I constructed the original matrix, it was equal by construction.
16:48
I looked at the maximum number of rows I get. And I told you this is what I constructed. And therefore, by definition, this is B of N and k.
16:55
Here, I obtained this in a special way. I took out a guy from the other matrix, and did that.
17:01
I am not sure that this is the best way to maximize the number of rows. At least it's conceivably not.
17:06
But for sure it's at most B of N minus 1 and k, because that is the maximum number. I am safe saying that it's less than or equal to.
17:13
So I have the first one.
17:19
Now, let's try to estimate beta by itself. This is the more subtle argument. In this case, we are going to focus now on the second part
17:28
only, the S_2 part. The guys that appear twice in the big matrix.
17:34
So let's focus on them. Now, when I focus on them, these guys are very easy to analyze.
17:41
They are here and here exactly the same. This block is identical to this block.
17:48
The interesting thing, when I look at these guys, is that I am going to be able to argue that these guys have a break point of k minus 1, not k.
18:00
The argument is very cute. Let's say that you have all possible patterns on k minus 1 guys, in this
18:07
small matrix. First, I have to kill these. These are not different guys, because these are identical to these.
18:14
So let me reduce it to the guys that are patently different. I'm now looking at this matrix. I am claiming that k minus 1 is a break point here.
18:22
Why is that? Because if you had k minus 1 guys here, where you get all possible patterns, then by adding both copies, +1 and -1, and adding x_N, you
18:34
will be getting k columns overall that have all possible patterns, which you
18:40
know you cannot have because k is a break point for the whole thing. So now I'm taking advantage of the fact that these guys repeat.
18:47
It's very dangerous to have k minus 1 guys, because now I have the k that I know doesn't exist.
18:53
Let's do it illustratively. Here is a pattern here. You add the +1 extension and the -1 extension,
19:00
by taking this column. If you get all possible patterns on k minus 1, and you add this guy, then you
19:07
have both patterns here, and you will end up with all possible patterns on k points on the overall matrix.
19:12
That enables me to actually count this in terms of B of N and k, again,
19:19
with the proper values of N and k. We can say that beta is less than or equal to-- again, less than or
19:24
equal to because I obtained this matrix by lots of eliminations. I didn't do it deliberately to maximize the number, so I don't know
19:30
whether it's the maximum. But I sure know that it's less than or equal to the maximum, by the definition of what a maximum is.
19:36
And that would be of what? I have N minus 1 point and I argued for a break point of k minus 1.
19:44
So I end up with this fellow. Both arguments are very simple.
19:49
Now, we pull the rabbit out of the hat! You put it together. What do we have?
19:55
This is the full matrix. The first item was just calling things names, the number of rows in the big
20:02
matrix is B of N and k, by definition-- by construction. I organized it such that there is alpha, and there is beta, and there is
20:10
another beta, so this one is the first result I got, which is B of N and k equals alpha plus 2 beta.
20:16
What else did I get? I got that alpha plus beta is at most B of N minus 1 and k.
20:24
That was the first slide of the analysis. We have seen that. So this basically takes this matrix, and does an analysis on it.
20:35
And it has a break point k, because k will be inherited when you go to the bigger one. That's what we did.
20:40
The other one is, beta is less than B of N minus 1 and k minus 1. And this is the case where I only looked at this guy, and now I have to
20:48
be more restrictive in terms of all possible patterns, because I have an extension to add, and I would be violating the big constraint.
20:54
So I ended up with this being less than or equal to B of N minus 1 and k minus 1.
20:59
Anybody notice anything in this slide? How convenient! I have alpha plus 2 beta there, and I have alpha plus beta
21:07
on one, and beta on one. If I add them, I am in business. I can actually now relate B of N and k to other B of N and k, and alpha and
21:17
beta are gone. B of N and k, now I know, has to be at most this fellow.
21:25
So you can see where the recursion is going. Now I know that this property holds for the B of N and k.
21:32
And now all I need to do is solve it, in order to find an actual numerical value for B of N and k.
21:37
And that numerical value will serve as an upper bound for any growth function of a hypothesis set that has a break point k.
21:45
Let's do the numerical computation first.
21:53
I have this recursion, and I can see that, from smaller values of N and
21:58
K, I can get bigger values, or I can get an upper bound on bigger values. Let's do it in a table.
22:06
Here is a table. Here is the value of N-- 1, 2. This is the number of points, the number of columns in the matrix.
22:13
And this is k. This is the break point I am talking about. So this will be-- there's a break point of 1, break point 2, break point
22:19
3, et cetera. And what I'd like to do here, I'd like to fill this table with an upper bound
22:24
on B of N and k. I'd like to put numbers here, that I know that B of N and k can be at most that number.
22:31
And we can construct this matrix very, very easily having this recursion. Here's what we do.
22:38
First, I fill the boundary conditions. Let's look at this. Here it says that there is a break point of 1.
22:47
I cannot get all possible patterns on one point. Well, what are all possible patterns on one point?
22:55
-1 and +1. It's one point. So I cannot get both -1 and +1. That's a pretty heavy restriction.
23:01
So I'm asking myself, let's say you have now N columns in the matrix. How many different rows can you get in that matrix, under that constraint?
23:11
Well, I'm in trouble. Because if I have the first pattern, and then I put a second pattern, the
23:18
second pattern must be different from the first one in at least one column. That's what makes it different.
23:24
If it's identical in every column, then it's not a different pattern, right? So you go to that point, where it's different.
23:30
And unfortunately, for that point you get both possible patterns.
23:36
So we are stuck. We can only have one pattern under this constraint. Hence, the 1's--
23:42
1, 1, 1, 1, 1. That's good. Now, in the other direction, it's also easy.
23:51
In this case, it's 2. It's very easy to argue. Now, I am taking the case where I have only one column.
23:58
So I'm asking myself, how many patterns I can get for one column. Well, the most is 2.
24:04
Why am I getting 2's here? Because in the upper diagonal of this table, the constraint I
24:12
am putting is vacuous. Here, for example, I am telling you how many different patterns can you
24:19
get on one point, such that no four points have all possible patterns.
24:26
Four points, what are you talking about? You have only one point. So that's no constraint at all.
24:32
Therefore, it doesn't restrict the choices, and the maximum number is the maximum number I would get unrestricted, which happens to be 2.
24:39
If I have one point, I get two patterns. That's why you have the 2's sitting here. Now, I covered the boundary conditions, and that's really all I need to
24:48
complete the entire table, given the nature of the constraint I have. Why is that?
24:55
Because that constraint looks like this. If you know the solid blue guys, I will tell you the empty blue guy.
25:03
Because this would be-- look at N and k. This is N and k. This would be N minus 1 and k.
25:12
This would be N minus 1 and k minus 1. That's exactly what this says.
25:19
So if I have these two points, I can get a value here, which will be an upper bound on this fellow.
25:25
Let us actually go through this table, and fill it up. The first guy I'm going to take is this 1 and 2.
25:32
According to this shape, I might be able to get this fellow. What would that fellow be?
25:40
3, right? You just add the two numbers. How about the next guy?
25:46
Anybody has a guess here? OK, 4.
25:53
And then? A bunch of 4's. Always get 2's. I am actually happy about this because you see that when k grows big, much
26:02
bigger than N, as we said, the constraint is vacuous. So I should be getting all possible patterns on the number of points I have.
26:08
And as you can see, for 1, I get the 2's. For 2, I will get eventually the 4's. And for 3, it will be the 8's.
26:14
So that is very nice. Let's go over the next row. Can I solve this one? Now that I got this one, I can become more sophisticated and get this one.
26:23
See where this came from? How about the next one, what would that be? That should be 7, right?
26:28
8. A bunch of 8's.
26:35
This is kind of fun. And you can fill up the entire table.
26:42
So we have it completely solved, numerically. It would be nice to have a formula, which we will have in a minute.
26:47
But numerically, we will have that. Now, let me highlight one guy.
26:53
Do you see anything that changed colors? I claim that you have seen this before.
26:59
That's the puzzle.
27:06
You had three points. Your break point was 2.
27:12
And now we know for a fact that the maximum number you can get is 4, without having to go through the entire torture we went
27:19
through last time. Can we try this? Can we try that? Oh, I am violating-- You don't have to do that. Here are the numbers, just by computing a very simple recursion.
27:28
Now, let's go for the analytic version of that. What I'd like to do, I'd like to find a formula that computes this
27:35
number outright. I don't have to go through this computation numerically.
27:40
So let's do that. This is the analytic solution for B of N and k.
27:46
Again, this is the recursion. And now we have a theorem.
27:52
Yeah, when you're doing mathematical theory, you have to have theorems. Otherwise, you lose your qualifications!
27:58
What does the theorem say? It tells you that this is a formula that is an upper bound
28:06
for B of N and k. What is this formula? This is N choose i, the combinatorial quantity.
28:13
And you sum this up from i equals 0 to k minus 1. So both N and k appear. N appears as the number here, and k appears as the limit for the index of
28:22
summation-- appears as k minus 1. This quantity will be an upper bound for B of N and k.
28:29
You can now, if you believe that, which we will argue in a moment, you compute this number.
28:34
And that will be an upper bound for the growth function of any hypothesis set that has a break point k, without asking any questions whatsoever about
28:43
the hypothesis set or the input space. It shouldn't come as a surprise that this quantity is right, because if
28:51
you look at this, this is really screaming for something binomial or
28:56
combinatorial. Clearly, it will come out one way or the other. But why is it this way? Well, what we are going to show, we are going to show that this is exactly
29:04
the quantity we computed numerically for the other table. And we are going to do it by induction.
29:10
So the recursion we did, we are just going to do it analytically. How do you do that?
29:16
You start with boundary conditions. What were the boundary conditions? We argued that this is, indeed, the value of B of N and k.
29:22
And hence, an upper bound on it, from the last slide. Now we want to verify that this quantity actually returns those
29:30
numbers, when you plug in the value N equals 1 or k equals 1.
29:36
How do I do that? You just do it. Just plug in, and it will come out. I'm not going even to bother doing it.
29:42
It's a very simple formula. You just evaluate it, and you get that. The interesting part is the recursion.
29:49
I would like to argue that if this formula holds for the solid blue
29:57
points, then it will also hold for this guy. And then by induction, since it holds for all of these guys, I can just do
30:04
this step by step and fill the schedule, with the truth of this being
30:09
the correct value for the numbers that appear here.
30:15
Everybody is clear about the logic of it? So let's do the induction.
30:21
We have the induction step. We just want to make clear what the induction step is.
30:26
You are going to assume that the formula holds for this
30:33
point and this point. So indeed, if you plug in the values for N and k, which here, N minus 1 and k
30:39
minus 1, and here it would be N minus 1 and k, you plug it into that particular formula.
30:45
Then the numbers will be correct. That's the assumption. And then you prove that, if this is true, then the formula will hold here.
30:53
That's the induction step. Fair enough. So let's do that.
30:59
This is the formula for N and k. You just need to remember it. N appears here and k appears here.
31:06
Minus 1 is an integral part of the formula. This is the value for k, not for k minus 1.
31:11
The value, for k, happens to be the sum from i equals 0 to that k, minus 1. So this is the formula that is supposed to be here.
31:21
And we would like to argue that this is equal to-- what is this one?
31:26
This one is for N minus 1, and still k.
31:32
So this would be-- I am moved from here to here. So this will be the value here.
31:38
And what is the other guy? That will be the value for N minus 1.
31:43
And now it's for k minus 1, because you still take the other minus 1. It becomes k minus 2. This part belongs here.
31:51
So this is the induction step. We don't have it yet. That's what we want to establish. So let me put a question mark to make sure that we haven't
31:57
established it yet. What I am going to do, I am going to take the right-hand side, and keep
32:04
reducing it, until the left-hand side appears. That's all. And then we'll be done with the induction step.
32:10
And since we have the boundary conditions, we will have proved the theorem we asserted.
32:16
The first thing I am going to do, I am going to look at this fellow. And I notice that the index goes here from i equals 0 to k minus 1.
32:24
Here, it goes from i equals 0 to k minus 2. I'd like to merge the two summations. So in order to merge the two summations, I will make them the same
32:31
number of terms, first. Very easy. I will just take the zeroth term, which would be N minus 1 choose
32:38
0, which is 1, out. And now the summation goes from i equals 1 to k minus 1.
32:45
Now, I go to the other guy and do this. What did I do?
32:51
I just changed the name of the dummy variable i. I wanted the index to go from 1 to k minus 1, in order to be
32:57
able to merge it easily. Here, it goes from 0 to k minus 2. So what do I do? I just make this i, and make this i minus 1.
33:04
So i minus 1 goes from 0 to k minus 2, as this i used to. Just changing the names.
33:10
And now, having done that, I am ready to merge the two summations. And they are merged.
33:19
Now, I would like to be able to take this, and produce one quantity.
33:25
And you can do it by brute force. This is no mysterious quantity. This is what? This is N minus 1 times N minus 2 times N minus 3, i terms,
33:33
divided by i factorial. And this one applies the same thing. So you end up with something, and then you do all kinds of algebra, and it
33:43
looks familiar. And then you reduce it to another quantity. So there's always an algebraic way of reducing it. But I am going to reduce it with a very simple combinatorial argument.
33:51
I am going to claim that this is-- the 1 remains the same. And this actually, the whole thing here reduces to N choose i.
33:58
So these two guys become this one. Instead of doing the algebra, I am going to give you
34:05
a combinatorial argument. That is, this quantity is identical to N choose i.
34:11
Let's say that I am trying to choose 10 people from this room.
34:17
And let's say that the room has N people. There are N people. How many ways can you chose 10 people out of this room?
34:28
That is N choose 10. Let's put this on the side.
34:34
Here is another way of counting it. We can count the number of ways you can pick 10 people, excluding me, plus
34:44
the number of ways you can pick 10 people, including me.
34:50
Right? These are disjoint, and they cover all the cases. Let's look at excluding me.
34:55
How many ways can you pick 10 people from the room, excluding me?
35:00
Well, then you are picking the 10 people from N minus 1. I am the minus 1.
35:06
So that would be N minus 1 choose 10. Put this in the bank.
35:13
How many ways can you pick 10 people, including me? Well, you already decided you are including me, so you are only deciding
35:20
on the 9 remaining guys. So that would be N minus 1 choose 9.
35:27
So we have N minus 1 choose 10, plus N minus 1 choose 9, that equals the
35:32
original number, which was N choose 10. Look at this. What do we have?
35:38
This is excluding me. This is including me.
35:45
And this is the original count. So it's a combinatorial identity, and we don't have to go through the torture of the algebra in order to prove that it's exactly the same.
35:55
Now, I go back. I look, this goes from i equals 1 to k minus 1. I have this 1, so I conveniently put it back, and get this formula.
36:04
Have you seen it before? Yeah, it looks familiar. Oh, this is the one we want to prove.
36:09
So it means that we are done. That's it. We have an exact solution for the upper bound on B of N and k.
36:20
Since we spent some time developing it, let's look at it and
36:25
celebrate it, and be happy about it. First thing: yes, it's a polynomial, because all of this torture was to get
36:32
a polynomial, right? If we did all of this, and it's perfect math, and the end result was not a polynomial, then we are in trouble.
36:39
Because although the quantity is correct, it's not going to be useful in the utility that we are aiming at.
36:45
So why is it polynomial? Remember that for a particular hypothesis set, the break point is
36:51
a fixed point. It doesn't grow with N. You ask in a hypothesis set, can I get all possible dichotomies on four points?
36:59
That's a question for the perceptron. No. Then 4, in the perceptron, is a break point.
37:04
Now, I can ask myself what the perceptron does on 100 points. And the break point is still 4, just a constant.
37:11
You give me a hypothesis set, I give you a break point. That's a fixed number.
37:17
So according to our argument now, the growth function for a hypothesis set
37:23
that has a break point k is less than or equal to the purely combinatorial
37:29
quantity, B of N and k, which is defined as the maximum such number of dichotomies you can get, under the constraint that k is a break point.
37:38
And that was less than or equal to the nice formula we had. So we can now make this statement.
37:44
You go in a real learning situation. Let's say you have a neural network making a decision, and you tell me the
37:50
break point for that neural network is 17. I don't ask you what is a neural network, because we don't know yet, so
37:57
you don't have to know. I don't ask you what is the dimensionality of the Euclidean space you are working on. You told me 17.
38:02
Your growth function of your neural network that I don't know, in the space that I don't know, happens to be less than or equal to that, and I
38:10
know that I'm correct. So is this quantity polynomial in N?
38:16
That's what we need. Because remember, in the Hoeffding, there was a negative exponential in N. If we get this to be polynomial in N, we are in business.
38:24
Well, any one of those guys is what? N times N minus 1 times N minus 2, i times, divided by i factorial.
38:33
i factorial doesn't matter, it's a constant. So you basically get N multiplied by itself a number of times, i times, for
38:38
the i-th term. The most that N will be multiplied by itself is when you get to i equals
38:44
k minus 1, the maximum. And then N will be multiplied by itself k minus 1 times.
38:49
Therefore, the maximum power in this quantity is N to the k minus 1.
38:59
This comes from N times N minus 1 times N minus 2 times-- k times, that corresponds to the case where i equals k minus 1.
39:07
When you get N choose k minus 1, that's what you get. Anything else, will give you a power of N, but it's a smaller power.
39:14
This is the most you will have. What do we know about this fellow k? We know it's just a number.
39:20
It's a constant. It doesn't change with N. And therefore, this is indeed a polynomial in N. And we have achieved what we set out to do.
39:32
That is pretty good. Let's take three examples, in order to make this relate to experiences we
39:40
had before. This is the famous quantity by now.
39:46
You know it by heart. I have the N. I remember k. I have to put minus 1. And that is the upper bound for anything that has a break point k.
39:53
Now, let's take hypothesis sets we looked at before, for which we computed
39:58
the growth function explicitly, and see if they actually satisfy the bound. They had better, because this is math.
40:04
We proved it. But just to see that this is, indeed, the case. Positive rays.
40:10
Oh, remember positive rays from some time ago? We have one dimension, so just the real line.
40:15
Then we take from a point on, it goes to +1. And we said that the whole analysis here is exactly to
40:22
avoid what I just did. You don't have to tell me what is the input. You don't have to tell me-- you just have to tell me what?
40:29
What is the break point? That's all we want.
40:34
you can call it positive rays. You can call it George, I don't care! It has a break point of 2.
40:39
That's what I pull. We did compute the growth function for the positive rays.
40:46
We did it by brute force. We looked at it, and we see what the patterns are, and did a combinatorial argument. And we ended up, that the growth function for this guy is N plus 1.
40:54
Let us see if this satisfies the bound. This is supposedly less than or equal to.
41:00
And you substitute here for N, which is the number here. And the break point is k.
41:05
So you're summing, from i equals 0 to 1, this quantity. You have N choose 0, also known as 1.
41:14
Plus N choose 1, also known as N. That's it. So you get this to be less than or equal to--
41:21
wow! Look at the analysis we did to get the N plus 1.
41:27
And we get exactly the same. With all the bounds and-- we think that there is a big slack here.
41:32
But here, actually it's exactly tight. We get the same answer exactly, without looking at anything of the geometry of
41:39
what the hypothesis set was. Let's try another one. Maybe we'll continue to be lucky.
41:44
Positive intervals. Yeah, I remember those were the more sophisticated-- oh, I'm sorry. I am not supposed to ask any questions about the hypothesis.
41:51
I'm asking about the break point only. I remember now. So tell me what the break point is.
41:56
That was k equals 3. And we did compute the growth function.
42:01
Remember, this one was a funny one. We're picking two segments out of N plus 1, and then adding the 1.
42:07
So we ended up-- this would be the formula. What would be the bound according to the result we just had?
42:14
This would be again, this formula. And now k equals 3. So I have N choose 0 plus N choose 1 plus N choose 2.
42:23
I get 1 plus N plus something that has squared terms. And I do the reduction and what do I get?
42:31
Boring, boring. I seem to be getting it all the time. It doesn't happen this way.
42:37
It will always happen that it's true. But there will be a slack in many cases.
42:44
So, we verified it. We are very comfortable now with the result. Let's apply it to something where we could not get the growth function.
42:53
Remember this old fellow? Well, in the two-dimensional perceptron, we went through a full argument just to prove that the break point is 4.
43:02
But we didn't bother go through a general number of points N, and ask ourselves, how many hypotheses can the perceptron generate on N points?
43:09
Can you imagine the torture? We do this-- can I get this pattern-- And you have to do this for every N. So we didn't do it.
43:17
The growth function is unknown to us. We just know the break point. But using just that factor, we are able to bound the growth function
43:25
completely. And you substitute again with k equals 4. You get another term, which is cubic. And you do the reduction.
43:31
And lo and behold, you have that statement. That statement holds for the perceptrons in two dimensions.
43:37
And you can see that this will apply to anything. So now it was all worth the trouble, because now we have a very simple
43:44
characterization of hypothesis sets. And we can take this, and move to the other part.
43:51
Remember, this part, which has now disappeared is proving that it's polynomial. Proving that we are interested in the growth function.
43:59
If it wasn't polynomial, we wouldn't be interested in it. Now, this is an interesting quantity.
44:04
This one tells us that-- oh, and by the way, it's not only interesting. You actually can use it.
44:09
We can put it in the Hoeffding inequality, and claim that the Hoeffding inequality is true using the growth function.
44:16
Now let's see what we want, to remind you of the context of substituting for
44:24
the total number of hypotheses by the growth function. We wanted, instead of having this fellow--
44:31
this is Hoeffding, and this is the number of hypotheses using the union bound, which we said is next to useless whenever M
44:37
is big, or M is infinite. And instead of that, we wanted really to substitute for this
44:43
by the growth function. So this is what we are trying to do. We are trying to justify that instead of this, you can actually say that.
44:49
Well, it turns out that this is not exactly what we are going to say. We are going to modify the constants here for technical reasons that will
44:56
become clear. But the essence is the same. There would be the growth function here. It will be polynomial in N, and it will be killed by the negative
45:03
exponential, provided that there is a break point-- any break point.
45:09
Now, how are we going to prove that? We are going to have a pictorial proof.
45:16
What a relief, because I think you are exhausted by now. So the formal proof is in the Appendix.
45:22
It's six pages. My purpose of the presentation here is to give you the key points in
45:27
the proof, so that you don't get lost in epsilon's and delta's. There are basically certain things you need to establish.
45:34
And once you know that that's what you are looking for, you can bite the bullet and go through it line by line.
45:40
The two aspects are the following. Why did we do this growth function?
45:46
We used the growth function because it's smaller, so it will be helpful. But how could it possibly replace M?
45:53
Because M was assuming no overlaps at all in the bad regions. Remember?
45:58
So now that we know that there are overlaps, this will take care of it. The question is, how does the growth function actually
46:06
relate to the overlaps? You need to establish that. So this is the first one.
46:12
And when we establish that, we find that it's a completely clean argument at everything, except for one annoying aspect.
46:18
Growth function relates to a finite sample. So we will get a perfect handle on the E_in, the in-sample
46:24
error part of the deal. But in the Hoeffding inequality, there is this E_out. And E_out relates to the performance over the entire space.
46:32
So we are no longer talking about dichotomies, we are talking about full hypotheses. We lose the benefit of the growth function.
46:39
So what do we do about E out? That was a question that was asked last time. What to do about E_out, in order to get the argument to conform
46:48
while we are just using a finite sample, is the second step. After that, it's a technical putting it together, in order to get
46:54
the final result. That's the plan. But the proofs are pictures. So let's have a blank page.
47:01
And let's say you are an artist and this is your canvas.
47:06
It's a very special canvas. It's the canvas of data sets.
47:13
What is that? Every point here is an entire data set, x_1, x_2, up to x_N.
47:21
Fix N in your mind. So this is one vector. This is another vector. This is another vector.
47:26
And this canvas covers the entire set of possible data sets.
47:32
Now, why am I doing this space? Well, I am doing this space because the event of being good or bad, whether
47:39
E_in goes to E out, depends on the sample. Depends on the data set. For some data sets, you will be close to the E_out.
47:46
For some data sets, you are not going to be close. So I want to draw it here, in order to look at the bad regions and the
47:53
overlaps, and then argue why the growth function is useful for the overlaps. Now, we assume that there's a probability distribution.
47:59
And for simplicity, let's say that the area corresponds to the probability. So the total area of the canvas is 1.
48:07
Now, you look at the event, the bad event, that E_in is far away from E_out.
48:12
And let's say that you paint the points that correspond to that event red.
48:17
So you pick-- is this data set good or bad? What does it mean, good or bad? I look at E_in in that data set, compare it to E_out on a particular
48:23
hypothesis, and then paint it red if it's bad. So I have a hypothesis in mind, and I am painting the points here red or
48:31
leaving them alone, according to whether they violate Hoeffding's inequality or not.
48:37
And I get this, just illustratively. And you realize that I didn't paint a lot of area.
48:43
And that is because of Hoeffding inequality. Hoeffding inequality tells me that that area is small.
48:49
So I'm entitled to put a small patch. Now we went from one hypothesis, which is this guy, to the case where we have
48:59
multiple hypotheses, using the union bound. So again, this is the space of data sets, exactly the same one.
49:05
And now, I am saying for the first hypothesis, you get this bad region.
49:11
What happens when you have a second hypothesis? Because I am using the very pessimistic union bound, I am hedging
49:18
my bets and saying you get a bad region that is disjoint.
49:23
Another hypothesis-- two of them. More.
49:30
More. Oh, no. We are in trouble. The colored area is the bad area.
49:37
Now the canvas is the bad area. That's why we get the problem with the union bound.
49:42
Because obviously, having them disjoint fills up the canvas very quickly. Each of them is small, but I have so many of them.
49:48
Infinity of them as a matter of fact. This will overflow. Well, no, it won't overflow. Just figuratively speaking.
49:54
So that's what I'm going to have. What is the argument we are applying now?
50:00
We are not applying the union bound. We are going to a new canvas. And that canvas is called the VC bound, as in Vapnik-Chervonenkis.
50:10
We'll see it in a moment. So what do you do? Your first hypothesis, same thing.
50:16
When you take the second hypothesis, you take the overlaps into consideration. So it falls here.
50:22
You get more. You get more. You get all of them.
50:30
It's not as good as the first one. I never expected it to be. But definitely not as bad as the second one, because now they are
50:37
overlapping. And indeed, the total area, which is the bad region-- something bad happening-- is a small fraction of the whole thing.
50:44
And I can learn. So we are trying to characterize this overlap.
50:50
That's the whole deal with the growth function. One way to do it is the one that I alluded to before. Study the
50:56
hypothesis set. Study the probability distribution. Get the full joint probability distribution of any two events
51:02
involving two hypotheses, and then characterize this. Well, good luck!
51:08
We won't do that. The reason we introduced the growth function, because it's an abstract quantity that is simple, and is going to characterize the overlaps.
51:16
The question is, how is the growth function going to characterize the overlaps? Here is what is going to happen.
51:24
I will tell you that if you look at this canvas, if any point gets
51:29
painted at all, it will get painted over 100 times.
51:36
Let's say that I have that guarantee. I don't know which hypotheses will paint it again. But any point that gets a red, it will get a blue, and a green-- 100 times.
51:46
If I tell you that statement, what do you know about the total area that is colored?
51:52
Now it's, at most, one hundredth of what it used to be. Because when I had them non-overlapping, they filled it over.
52:00
Now for every point that is colored, I have to do it 100 times. So I am overusing these guys, and these guys will have to shrink.
52:06
And I will get one hundredth of that. That is basically the essence of the argument. What the growth function tells you is that-- what is the growth function?
52:14
Number of dichotomies. If you take a dichotomy, this is not the full hypothesis, but the hypothesis on a finite set of points.
52:21
There are many, many hypotheses that return the exact same dichotomy.
52:28
Remember the gray sheet with the holes. Lots of stuff can be happening behind the sheet.
52:34
And as far am I am concerned, they are all the same dichotomy. So all of these guys will be behaving exactly the same way.
52:41
If one of them colored the point, the others will. This tells me that the redundancy is captured
52:48
by the growth function. That would be a very clean argument. And it would have been a very simple proof, except for one annoyance.
52:57
That the point being colored doesn't depend only on the sample, but depends also on the entire space.
53:03
Because the point gets colored because it's a bad point. What is a bad point? The frequency on the sample, that is patently on the sample, deviates
53:12
from E_out. Oh, E_out involves the entire hypothesis. If I have the gray sheet and the holes, I cannot compute E out.
53:21
I have to peel it off, look, and get the areas in order to get E out.
53:26
So the argument is great, as long as you can tell me how do I go around
53:31
the presence of E_out? And that's the second part of the proof.
53:39
What to do about E_out. The simplest argument possible.
53:44
That is really the breakthrough that Vapnik and Chervonenkis did. Back to the bin, just because it's an illustration of the binary case.
53:51
So here, we have one hypothesis. And we have E out, which is the overall, in the entire space--
53:59
the error in the entire space. We pick a sample, and then we get E_in, which is the value for the error
54:06
on this one. So we have seen this before. And we said this tracks this, according to the Hoeffding inequality.
54:12
And the problem is that when you have many, many bins, some of these guys will start deviating from E_out, to the level if you pick according to the
54:19
sample, you are no longer sure that you picked the right one, because the deviation could be big. That was the argument.
54:25
Now, I want to get rid of E_out. The way I am going to do it is this.
54:31
Instead of picking one sample, I am going to pick two samples,
54:38
independently. So obviously, they are not identical samples. Some of them are green or red, et cetera.
54:43
But they are coming from the same distribution. Now, let's see what is going on.
54:48
E_out and E_in track each other, because E_in is generated from this distribution.
54:55
Now, let's say I look at these two samples and give them names. I am going to call them E_in and E_in dash.
55:04
They're both in-sample. It happens to be a different sample. So I have two samples. I am going to call this E_in, and this E_in dash.
55:11
My question is, does E_in track E_in dash, if you have one bin?
55:19
Well, each of them tracks E_out, right? Because it was generated by it.
55:25
So consequently, they track each other. A bit more loosely, because you have now two ways of
55:32
getting the sample error. On the other hand, if I do two presidential polls--
55:38
one polling asks 3000 people. Another asks 3000 people. These are different 3000 people.
55:45
You fully expect that the result will be close to each other, right?
55:50
So these guys track each other. OK, fine. What is the advantage? The advantage is the following.
55:57
If I now have multiple bins, the problem I had here is reflected
56:03
exactly in the new tracking. When I had multiple bins, the tie between E_out and E_in became looser
56:11
and looser. Because I'm looking for worst case, and I might be unlucky enough, that the
56:16
tracking now lost the tightness that one bin with Hoeffding would dictate.
56:24
If I am doing multiple bins, and not looking at the bin at all, just looking at the two samples from each bin, they track each other, but they
56:32
also get loosely apart as I go for more. Let's say, I tell you this experiment.
56:40
You pick two samples. They are close in terms of the fraction of red.
56:45
If you keep repeating it, can you get one sample to be mostly red, and the other sample to be mostly green?
56:51
Yeah. If you are patient enough, it will happen. Exactly for the same reason, because you keep looking for it until it happens.
56:58
So the mathematical ramifications of multiple hypotheses happen here,
57:03
exactly the same way they happen here. The finesse now is that, if I characterize it using the two
57:10
samples, then I am completely in the realm of dichotomies.
57:15
Because now I'm not appealing to E_out at all. I am only appealing to what happens in a sample. It's a bigger sample.
57:21
I have 2N marbles now instead of N. But still, I can define a growth
57:27
function on them. And now the characterization is full, and I am ready to go. These are the only two components you need to worry about as
57:34
you read the proof. Now, let's put it together.
57:39
This is what we wanted. This is not true.
57:44
Don't hold this against me. And to make sure, this is not quite what we have. This would be direct substitution of the plain-vanilla growth function in
57:53
terms of M. We are not going to have that, but we are going instead to have this.
58:03
Lets look and compare. These look the same, except that this 2 became 4.
58:12
Is this good or bad? Well, it's bad. We want this probability to be small. Bad, but not fatal.
58:17
This one goes to here. I have twice the sample.
58:22
You know why I have 2 now. Because now I use the bigger sample for the argument, so I need 2N.
58:29
Oh, but all of this was about a polynomial and now I don't know whether this will be a polynomial. Yes, you do.
58:34
If it's polynomial in N, it's polynomial in N here. Because you get 2N to the k, then you get 2 to the k.
58:40
That's a constant. And you still get N to the k. So that remains a polynomial.
58:45
A bigger polynomial. I don't like it, but you don't have to like it. It just has to be true, and do the job we want.
58:53
And finally, you can see this is minus 2, which was a very helpful factor. This is in the exponent. 2 in the exponent goes a lot of mileage.
59:01
And now we knock it down all the way to 1/8. That's really, really bad news.
59:06
The reason this is happening is that, as we go through the technicalities of the proof, the epsilon will become epsilon over 2.
59:13
And then will become epsilon over 4, just to take care of different steps. And when you plug in epsilon over 4 here, you get epsilon squared over 16.
59:20
And so you get a factor of 1/8. That's the reason for it. So this is what we will end up with.
59:26
And you can be finicky and try to improve this constant a lot, but the basic message is that here is a statement that holds true for any
59:35
hypothesis set that has a break point. And this fellow is polynomial in N, with the order of the polynomial
59:44
decided by the break point. And you will eventually learn, because if N is big enough-- if I give you
59:49
enough examples-- using that hypothesis, you will be able to claim that E_in tracks E_out correctly.
59:56
This result, which is called the Vapnik-Chervonenkis inequality, is the most important theoretical result in machine learning.
1:00:06
On that happy note, we will stop here and take questions after a short break.
1:00:19
Let's start the Q&A. MODERATOR: First, a few clarifications from the beginning.
1:00:29
In slide 5, when you choose the N points, does it mean your data set is
1:00:40
of N points, or you just chose N points from the data set? PROFESSOR: When I apply this to an actual hypothesis set in an input space, then
1:00:50
these actually correspond to a particular set of points in that space. However, in the abstraction that just defines the function B, these are just
1:01:00
abstract labels. These are labels for which column I'm talking about.
1:01:06
So although I call them x_1 up to x_N-1, these are not really--
1:01:11
in the abstraction here, they don't correspond to any particular input space in mind. But when they do, they will correspond to a sample.
1:01:17
And I am supposed to pick the sample in that space that maximizes the number of dichotomies, et cetera, as we defined the growth function.
1:01:25
But it's a sample that I pick when I apply this to a particular input space and a hypothesis set.
1:01:30
MODERATOR: Also, there are some people asking--
1:01:36
they didn't understand exactly why alpha was different to beta. PROFESSOR: alpha is different from beta?
1:01:41
MODERATOR: Yeah. Why? PROFESSOR: Well, the short answer is that I never made the
1:01:47
statement that alpha is different from beta. I just didn't bother ascertain any relationship between alpha and beta.
1:01:54
I just called them names. If they happen to be equal, I am happy. If they happen to be unequal, I am happy. So all I'm doing here is just calling the guys that happen to have a single
1:02:02
extension, the number of them, calling it alpha. Calling the guys that happen to have double extension beta.
1:02:10
I don't know whether alpha is bigger than beta, or smaller than beta, in any particular case. And it doesn't matter as far as the analysis is concerned.
1:02:16
If I call them this way, then it will always be true that the total number of rows here is alpha plus beta plus beta, which is alpha plus twice beta.
1:02:24
So there is really no assertion about the relative value of alpha and beta.
1:02:29
MODERATOR: Moving on to the case where you show the break points, and
1:02:35
how it satisfies the bound. What happens if k equals infinity? No break points, basically.
1:02:44
PROFESSOR: This is for the positive rays and whatnot? MODERATOR: Yeah. So for example, if you had the convex sets.
1:02:50
PROFESSOR: k equals infinity means there is no break point. In that case, you don't have to bother with any of the analysis I did.
1:02:57
No break points means what? Means the growth function is 2 to the N for every N, right? We
1:03:02
just computed it exactly. If you want a bound for it, yes, it's bounded by 2 to the N. Not a polynomial.
1:03:09
So all of these cases, we're addressing the case where there is a break point, because that is the case where I can guarantee a polynomial.
1:03:15
And therefore, I can guarantee learning. That is the interesting case. If there is no break point, this theoretical line of analysis will not
1:03:23
guarantee learning. So if I have a hypothesis set that happens to be able to shatter every
1:03:28
set of points, I cannot make a statement using this line of analysis that it will learn.
1:03:34
And one example we had was convex sets. So convex sets have a growth function of 2 to the N. Well, it really is
1:03:41
a very pessimistic estimate here, because the points have to be really, really very funny. You have to build the pathological case, in order not to be able to learn.
1:03:48
And in many cases, you might be. But again, if I want a uniform statement based only on the break
1:03:54
point, this is the most I can say using this line of analysis.
1:04:04
MODERATOR: OK. Just a quick review. How is the break point calculated?
1:04:11
PROFESSOR: Calculated. The break point is-- this is the only time you actually need to visit the input space and the hypothesis set.
1:04:18
You basically-- you are sitting in a room with your hypothesis set. Someone gave you a problem for credit approval.
1:04:24
You decided to use perceptrons, and you decided to use a nonlinear transformation. And you do all of that, and you start programming it.
1:04:32
And you would like to know some prediction of the generalization performance that you are going to get.
1:04:38
So you go into the room, and ask yourself: for this hypothesis set, over this input space, what is the break point?
1:04:43
So now you have to actually go and study your hypothesis set. And then find out that using this hypothesis set, I cannot separate,
1:04:50
let's say, 10 points in every possible way. Very much along the argument we used for the perceptron in two dimensions.
1:04:56
We found out that we cannot separate four points in every possible way. But the good news is that, you don't have to do it anew, because for most of
1:05:04
the famous learning models, this has already been done. For the perceptrons, we will get an exact break point.
1:05:11
For any-dimensional perceptron. So 20-dimensional perceptron, here's the break point, and here's the growth function.
1:05:17
Or, here's the bound. Similarity from neural networks, there is a break point. Not exact estimate of the break point, but a bound on the break point.
1:05:25
And again, in most of these cases, bounds work, because we are always trying to bound above. And we have room to play with, because a polynomial is a polynomial
1:05:34
is a polynomial. So if you become a little bit sloppy and forget something, and the break
1:05:40
point-- you say 10 instead of 7-- it's not going to break the back of learning versus non-learning. It's just going to tell you more pessimistically, how much resources do
1:05:49
you need in order to learn? Which is more benign damage than deciding, oh, I cannot learn at all.
1:05:57
MODERATOR: OK. Can you come up with an example, where these bounds are not tight as here?
1:06:06
PROFESSOR: There's one case, which I could have covered but I didn't, where you take positive and negative rays.
1:06:13
So positive rays, you take the real line. And from a point on it's +1. Before, it's -1.
1:06:19
Positive and negative rays, it means you are also allowed to take rays that return +1 first, and then -1 later.
1:06:26
And the union of them is the model called positive and negative rays. It's a good exercise to do. Take that home and try to find, what is the break point?
1:06:34
And you'll find that although the break point for positive rays is 2, in this case the break point is actually 3.
1:06:39
And the reason is that for two points, now you can get everything because you know the ray can be here.
1:06:44
So they are both minus. The ray could be here. They are both plus. The ray could be here. It's minus plus.
1:06:50
But now, use the negative ray to get the +1, -1. So now you can shatter two points.
1:06:55
And you would fail only for the three points, when the middle guy is different, because you cannot get it this way. So you will get-- and the break point is 3.
1:07:02
When you do the break point of 3, you will get the bound, the blue bound here. You will get that to be squared. Pretty much like here, because we have a break point that corresponds
1:07:09
directly to squared. I don't care whether the 3 is coming from positive intervals, or coming from positive and negative rays.
1:07:15
It's 3. Therefore, the blue bound is quadratic. If you compute the number of dichotomies you can get, which is the
1:07:22
growth function, it will actually be linear. So there will be a discrepancy between linear for the exact estimate of the
1:07:27
growth function, to quadratic of the bound. So there are cases that you can come up with, easily. And as a matter of fact, this is the exception on this,
1:07:34
rather than the rule. In most of the cases, there will be a slack.
1:07:39
MODERATOR: And this question drives the point of the whole lecture. It says, we have been focusing on having E_in equal to E_out, or close
1:07:49
to E_out, not in the actual value of E_in.
1:07:54
So using our hypotheses, there are just as many percentage errors in the training data as the real data.
1:07:59
Why is that? PROFESSOR: This goes back to separating the question of learning
1:08:04
into the two questions. There was one question which was addressed now. We are trying to get E_in to track E_out.
1:08:11
Why do I need that? Because I don't know E_out, and I will not know E_out. That is simply an unknown quantity for me.
1:08:19
And I want to work with something to minimize. I cannot minimize something that I don't know. So if the theoretical guarantees tell me that E_in is a proxy for E_out, and
1:08:28
if I minimize E_in, E_out will follow suit. Then I can now work with a quantity that I know, and do it.
1:08:33
That's the first part, that they are tracking each other. The second part is the practical. Now, I am going to go and try to minimize E_in.
1:08:40
This is the second part of it. MODERATOR: Also, they're asking if--
1:08:46
can you clarify more, why is the VC dimension useful?
1:08:52
PROFESSOR: The VC dimension, as of now, is an unknown quantity. I didn't say that word "VC dimension" at all. I said every building block that will result in the definition.
1:09:00
However, the good news is, what is the title of the next lecture? The VC dimension. You will be completely content with everything you wanted to know about
1:09:09
the VC dimension, and weren't afraid to ask!
1:09:15
OK? MODERATOR: Yeah, the crowd is saying that they're still
1:09:21
digesting the lecture. PROFESSOR: OK. As I mentioned before, if you didn't follow this in real time, don't be
1:09:27
discouraged. It's actually very sweet material. And you can look at the lecture again. And you can read the proof.
1:09:33
And you can do all of the homework, until it settles in your mind. This is the most abstract, or the most theoretical, lecture
1:09:40
of the entire course. And if you get through this one, and you understand it well, you are in good shape as far as the rest of the course.
1:09:45
There will be mathematics, but it will be more friendly mathematics. Friendly, as in less abstract.
1:09:50
For someone who is not theoretically inclined, the more abstract the mathematics is, the less they can follow it, because they
1:09:58
cannot relate to it. So this one has the abstraction. The other mathematics will be much easier to relate to.
1:10:04
MODERATOR: What was wrong with the "not quite" expression on the last slide?
1:10:10
PROFESSOR: OK.
1:10:15
Basically, the top statement is simply false. It was my way of relating what I'm trying to do, to
1:10:24
what has already happened. There used to be M in place of the growth function. So the growth function is here.
1:10:32
There used to be M. So the easiest way for me to describe what is happening with the theory, is to tell you that you are going to take
1:10:37
M out, and replace it with this. As usual, it's not that easy. Even remember with the Hoeffding, when I complained about the 2
1:10:44
here and the 2 here? Well, you have to have them, in order for the statement to be true. So for the statement to be true, we needed to do some technical stuff that
1:10:51
really didn't change the essence of the statement here, but made it a little bit different by changing the constants.
1:10:58
And therefore, we have a proof for it. It holds. And it captures the essence of that. I just didn't want to bother telling you this because, if I told you this in
1:11:06
the first place, you would have been completely lost. Why 4? Why 2? What is this 1/8? And forget about the essence.
1:11:12
So the easiest way to do it, we are replacing it. This is not the final form, but we are replacing it. Until you get the idea: indeed, I can replace it.
1:11:19
But oh, in order to replace it, I need to have a bigger sample that we argued for. So I need 2N. Oh, and now the bigger samples are not tracking to each other as well as each
1:11:27
of them is tracking the actual out-of-sample error. So I need to modify these values, and so on.
1:11:33
So it becomes much easier to swallow. The technicalities will come in, in order to make the proof go through.
1:11:39
MODERATOR: OK. Can you review the definition of B of N and k? PROFESSOR: B of N and k.
1:11:48
Assume you have N points, and assume that k is a break point. So you're assured that no k points will have all
1:11:55
possible patterns on them. After these two assumptions, make no further assumptions.
1:12:01
You don't know where this came from. You don't know what space you are working with. You don't know what the hypothesis set is.
1:12:06
You just know that in your setup, when you get N points-- that is the N here--
1:12:11
and the break point is k-- that is k here. Under those conditions, can you bound the growth function?
1:12:18
Can you tell me that the growth function can never be bigger than something? That something is what I am calling B of N and k.
1:12:25
So what am I doing? I'm taking the minimal conditions you gave me. I have N points, and k is a break point. And asking myself: what is the maximum number of dichotomies you can possibly
1:12:34
have, under no other constraints, in order to satisfy these two constraints? And I'm calling this B of N and k.
1:12:41
Why did I do it? First, it's going to help, being an upper bound for any hypothesis set
1:12:46
that has a break point k, because it is the maximum. The second one, it's a purely combinatorial quantity.
1:12:53
So I have a much better chance of analyzing it, without going through the hairy details of input spaces, and correlation between events, and so on.
1:13:01
And that is indeed, what ended up being the case. We had a very simple recursion on it, and we found a formula for it.
1:13:06
And that formula now serves as an upper bound for the more hairy quantity, which is the growth function that is very particular to a learning
1:13:13
situation, an input space, and a hypothesis set. MODERATOR: Also, a particular question on the proof of B of N and k, the recursion.
1:13:20
Slide 5. The question is, why does k not change when going back from
1:13:29
N to N minus 1? PROFESSOR: OK.
1:13:35
Here, if you look at x_1, x_2, up to x_N, the disappearing x_N here, no k
1:13:43
columns can have all possible patterns. These k columns could involve the last column, and could involve only the
1:13:51
first N minus 1 columns. Just no k columns whatsoever can have all possible patterns.
1:13:57
So when I look at the reduced one, N minus 1, I know for a fact that no k columns of these guys can have all possible patterns.
1:14:05
Because that would qualify as k columns of the bigger one. So k doesn't really change.
1:14:11
The only time I had a different k is when I had a nice argument
1:14:17
that, if you have k minus 1 points which have all possible patterns on
1:14:22
the smaller set, then adding the last column will get us in trouble with k columns.
1:14:28
So for that, I needed an argument. But in general, when I take the statement on face value, k is fixed.
1:14:34
And the k columns could be anything. Could be involving the last column, or could be restricted to the first guy. Could be the first k columns, for all I care.
1:14:40
MODERATOR: How does this formalization apply to, say, a regression problem?
1:14:50
PROFESSOR: Again, this is all binary functions. So the classification of +1 and -1. And as I mentioned, the entire analysis, the VC analysis, can be
1:14:58
extended to real-valued functions. It's a very technical extension that, in my humble opinion, does not add to
1:15:04
the insight. And therefore, instead of doing that and going very technical, in order to
1:15:10
gain very little in terms of the insight, I decided that when I get to regression functions, I am going to apply a completely different approach,
1:15:17
which is the bias-variance tradeoff. It will give us another insight into the situation, and will tackle
1:15:22
the real-valued functions directly, the regression functions. And therefore, I think we'll have both the benefit of having another way of
1:15:28
looking at it, and covering both types of functions. MODERATOR: There's this person that says, I feel silly asking this, but is
1:15:36
the bottom line that we can prove learnability if the learning model cannot learn everything?
1:15:42
PROFESSOR: OK. We proved learnability under a condition about the hypothesis set.
1:15:49
When you say learning everything, you are really talking about the target function.
1:15:54
So the target function is unknown. What I am telling you here is that, if you tell me that there is a break
1:15:59
point, I can tell you that if you have enough examples, E_in will be close to E_out for the hypothesis you pick, whichever way you pick it.
1:16:09
It remains to be seen whether you are going to be able to minimize E_in, to a level that will make you happy.
1:16:16
I will never know that until you start minimizing. So if the target function happens to be extremely difficult, or completely
1:16:22
random-- unlearnable, you are not going to see this in the generalization question. The generalization question is independent of the target function.
1:16:29
I didn't bring it up here at all. It has to do with the hypothesis set only. The target function will come in-- if I get E_in to be small, E_out
1:16:38
will be small. I know that from the generalization argument that I made. Can I get E_in to be small? If the target function is random, you will get a sample that that is
1:16:46
extremely difficult to fit. And you are not going to be able to get E_in to be small. But at least, you will realize that you could not learn, in
1:16:55
that particular case. And in another target function, you will realize that you can learn, because E_in went down.
1:17:01
So the question of whether I can learn or not, the generalization part of it
1:17:06
is independent of the target function. The second question is very much dependent on the target function, but
1:17:12
the good news is that it happens in sample. I can observe it, and realize how well, or not so well, I learned.
1:17:18
MODERATOR: Also, going back to a previous question, does this also generalize
1:17:25
to multi-class problems? PROFESSOR: Basically, there is no restriction on the
1:17:31
inputs or the outputs. There is a counterpart. And instead of saying break point, what is a break point?
1:17:36
And dichotomies, they are not really dichotomies. You have real values. You have no real values, so there are technicalities to be done in
1:17:41
order to be able to reduce them to this case. But the same principle applies, regardless of the type of
1:17:46
function you have. MODERATOR: I think that's it. PROFESSOR: Very good. Thank you, and we'll see you next week.